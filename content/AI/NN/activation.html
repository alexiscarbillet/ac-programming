<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <title>Neural Network Activation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Activation for neural network">
    <meta name="keywords" content="alexis carbillet, carbillet, artificial intelligence, python, neural network, activation">
    <meta name="author" content="Alexis Carbillet ">
 
    <!-- Control appearance when share by social media -->
    <meta property="og:title" content="Activation for neural network" />
    <meta property="og:description" content="Activation for neural network" />
    <meta property='og:url' content="https://www.ac-programming.com/content/NN/activation.html" />
    <meta property="og:type" content="website" />

    <!-- Bootstrap core CSS -->
    <link href="../../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

	<!-- Icon -->
	<link rel="shortcut icon" href="../../../img/logo/logo.png" />
	
    <!-- Custom fonts for this template -->
    <link href="../../../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="../../../css/structure.css" rel="stylesheet">

	<!-- Choice of languages -->
	<link rel="alternate" hreflang="x-default" href="https://www.ac-programming.com/" />


  </head>

  <body id="page-top">





    <!-- Navigation --> 
    <nav class="navbar navbar-expand-lg fixed-top" id="mainNav">
      <div class="container">
		<img src="../../../img/logo/logo.png" width="5%" style="margin-right: 2%;" alt="">
		<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
			<li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#article">Article</a>
            </li>
			<li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#python">Comparison</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
            </li>
			<li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="../../../index.html#advanced">Main</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>



    <!-- Header -->
    <header class="masthead">
	  <div class="container">		
		  
	  </div>
    </header>
	
<!-- ======================== ARTICLE SECTION ============================ -->

	<section id="article">
      <div class="container">
        <div class="row">
		  <div class="col-xs-12 col-md-12 col-sm-12 col-lg-12 text-center">
			<h2 class="section-heading text-uppercase">Neural Network Activation</h2>
		  </div>
		  <div class="col-xs-12 col-md-12 col-sm-12 col-lg-12 text-left">
		  <p>Activations for neural networks are essentially the "firing" or "activation" levels of individual neurons in a neural network. Just like how neurons in our brain transmit signals when they're active, neurons in a neural network also produce outputs when they receive inputs.</p>
		  <p>Each neuron in a neural network takes in a set of inputs, performs some computations on them, and then produces an output. This output is determined by applying an activation function to the weighted sum of the inputs. The activation function determines whether the neuron should "fire" or be active based on the computed value.</p>
		  <p>The purpose of using activation functions is to introduce non-linearities into the network, enabling it to learn and model complex patterns and relationships in the data. Without activation functions, neural networks would simply be a series of linear operations, which would severely limit their representational power.</p>
		  <p>Different activation functions have different properties and behaviors. Some common activation functions include the sigmoid function, which squashes the input into a range between 0 and 1, and the rectified linear unit (ReLU) function, which sets negative inputs to 0 and keeps positive inputs as they are.</p>
		  <p>By applying activation functions, neural networks are able to transform and process inputs in a non-linear manner, allowing them to capture intricate patterns and make more accurate predictions or classifications.</p>
		  </div>
		</div>
      </div>
    </section>

<!-- ======================================================================= -->

<!-- ======================== PYTHON SECTION ============================ -->

	<section class="bg-light" id="python">
      <div class="container">
        <div class="row">
		  <div class="col-xs-12 col-md-12 col-sm-12 col-lg-12 text-center">
			<h2 class="section-heading text-uppercase">Activations Comparison</h2>
		  </div>
		  <div class="col-xs-12 col-md-12 col-sm-12 col-lg-12 text-left">
			<p>Let's compare some commonly used activation functions in neural networks:</p>
			<pre><p>
			1. Sigmoid Activation Function:
			   - Formula: f(x) = 1 / (1 + e^(-x))
			   - Range: Outputs are in the range [0, 1]
			   - Properties:
				 - Smooth and continuous function
				 - Squashes the input into a sigmoid shape, emphasizing extreme values
				 - Useful for binary classification problems where the output needs to represent probabilities
			   - Drawbacks:
				 - The gradient vanishes for very large or very small inputs, which can hinder learning in deep networks
				 - Outputs are not zero-centered, which may slow down gradient-based optimization algorithms
				 
			2. Rectified Linear Unit (ReLU) Activation Function:
			   - Formula: f(x) = max(0, x)
			   - Range: Outputs are in the range [0, +∞]
			   - Properties:
				 - Simple and computationally efficient function
				 - Sets negative inputs to 0, keeping positive inputs unchanged
				 - Allows for faster training convergence compared to sigmoid-like functions
				 - Often used in deep learning models
			   - Drawbacks:
				 - Not differentiable at x = 0, which can cause issues for some optimization techniques
				 - Can result in dead neurons (neurons that never activate) if initialized with very negative weights

			3. Hyperbolic Tangent (Tanh) Activation Function:
			   - Formula: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
			   - Range: Outputs are in the range [-1, 1]
			   - Properties:
				 - Similar to the sigmoid function, but outputs are zero-centered
				 - Saturates the output for extreme inputs, similar to the sigmoid function
				 - Useful for classification problems where the output needs to represent negative and positive values
			   - Drawbacks:
				 - The gradient still suffers from vanishing gradients for very large or very small inputs

			4. Linear Activation Function:
			   - Formula: f(x) = x
			   - Range: Outputs are in the full range of real numbers (-∞, +∞)
			   - Properties:
				 - Simplest activation function, as it performs a linear transformation of the input
				 - Preserves the magnitude and sign of the input
				 - Often used in regression problems or as the final layer activation for certain tasks
			   - Drawbacks:
				 - The network becomes a linear model, limiting its ability to learn complex patterns
			</p></pre>
			<p>These are just a few examples of activation functions, and there are many others available. The choice of activation function depends on the specific problem, network architecture, and training requirements. Experimentation and tuning are typically performed to find the most suitable activation function for a given task.</p>
		  </div>
		</div>
      </div>
    </section>

<!-- ======================================================================= -->

<!-- ========================== CONTACT SECTION ============================== -->

<iframe id="contact" src="../../../contact.html"  frameborder="0" scrolling="no" style="border: none; width: 100%; height: 140vh; max-height: 800px;"></iframe>

<!-- ======================================================================= -->


<!-- ======================= FOOTER SECTION ================================ -->
    <footer>
      <div class="container">
        <div class="row">
			<div class="col-xs-3 col-md-3 col-sm-3 col-lg-3">
        <iframe src="../../../copyright.html" frameborder="0" style="padding: 0%; max-height: 25px;"></iframe>
			</div>
			<div class="col-xs-6 col-md-6 col-sm-6 col-lg-6 text-center">
			</div>
			<div class="col-xs-1 col-md-1 col-sm-1 col-lg-1"></div>
			<div class="col-xs-2 col-md-2 col-sm-2 col-lg-2">
			<a href="../../../index.html"><span class="copyright" style="margin-right: 2%;">Blog</span></a>
			</div>
		</div>
		<div id="particles-js" style="height:50px;"></div>
      </div>
    </footer>

<!-- ======================================================================= -->

<!-- ======================== JAVASCRIPT SECTION =========================== -->



    <!-- Bootstrap core JavaScript -->
    <script src="../../../vendor/jquery/jquery.min.js"></script>
	<script src="../../../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="../../../vendor/jquery-easing/jquery.easing.min.js"></script>
	
    <!-- Custom scripts for this template -->
    <script src="../../../js/structure.js"></script>
	
	<!-- Particles part -->
	<script src="../../../js/particles.js"></script>
	<script src="../../../js/app.js"></script>
	
    <!-- Activate the bootstrap tooltip, must be after jQuery load -->
    <script>
    $(function () {
      $('[data-toggle="tooltip"]').tooltip();
    })
    </script>
	

<!-- ======================================================================= -->
  </body>

</html>
